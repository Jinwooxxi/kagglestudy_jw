{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Porto_1st_kernel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3Szw+tc9f75sjkMpN7hX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinwooxxi/kagglestudy_jw/blob/main/Porto/Porto_1st_kernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAmqROsUqzVC"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook aims at getting a good insight in the data for the PorteSeguro competition. Besides that, it gives some tips and trick to prepare your data for modeling. \n",
        "\n",
        "1. Visual inspection of your data\n",
        "2. Defining the metadata\n",
        "3. Descriptive statistics\n",
        "4. Handling imbalaced classed\n",
        "5. Data quality checks\n",
        "6. Exploratory data cisualization\n",
        "7. Feature enginneering\n",
        "8. Feature selection\n",
        "9. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJENi32QrcHJ"
      },
      "source": [
        "# Loading packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyzgqlaKpqIw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pd.set_option('display.max_columns', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf5qAc9yrfio"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcKRA_qIn0bm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83p9Hgo6pnQT"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/porto/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/porto/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ5A_ExYrn8y"
      },
      "source": [
        "# Data at first sight\n",
        "\n",
        "Here is an excerpt of the data description for the competitons:\n",
        "\n",
        "* Features that belong to similar groupings are tagged as such in the feature names (e.g, ind, reg, car, calc).\n",
        "* Featuer names include the postfix bin to indicate binary features and cat to indicate categorical features.\n",
        "* Feautures without these designations are either continuous or ordinal.\n",
        "* Values of -1 indicate that the feature was missing from the observation.\n",
        "* The target columns signifiies whether or not a claim was filed for that policy holder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hr60ZnrqwF0"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXrK3U1rnng"
      },
      "source": [
        "train.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erb5ObVsszST"
      },
      "source": [
        "We indeed see the following\n",
        "\n",
        "* binary variables\n",
        "* categorical variables of which the category values are integers\n",
        "* other variables with integer or float values\n",
        "* variables with -1 representing missing values\n",
        "* the target variable and an ID variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnzuPjmUrnk6"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJBcuWnrniO"
      },
      "source": [
        "train.drop_duplicates()\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkOVea7Brnfn"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3THgOzwetQx1"
      },
      "source": [
        "So later on we can creat dummy cariables for the 14 categorical variables. The *bin* variables are already binary and do not need dummification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Ewk4IMrndL"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfwWTHVRtzw4"
      },
      "source": [
        "Again, with the info() method we see that the data type is integer or float. No null values are present in the data set. That's normal because missing values are replaced by -1. We'll look into that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMscutGzt1sR"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "To facilitate the data management, we';; store meta-information about the variables in a DataFrame. This will be helpful when we want to select spcific variables for analysis, visualization, modeling..\n",
        "\n",
        "Concretely we will store:\n",
        "* role : input, ID, target\n",
        "* level : nominal, interval, ordinal, binary\n",
        "* keep : True, False\n",
        "* dtype : int, float, str"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL8r3r6kuilG"
      },
      "source": [
        "data = []\n",
        "\n",
        "for f in train.columns:\n",
        "  # Defining the role\n",
        "  if f == 'target':\n",
        "    role = 'target'\n",
        "  elif f == 'id':\n",
        "    role = 'id'\n",
        "  else:\n",
        "    role = 'input'\n",
        "\n",
        "  # Defining the level\n",
        "  if 'bin' in f or f == 'target':\n",
        "    level = 'binary'\n",
        "  elif 'cat' in f or f == 'id':\n",
        "    level = 'nominal'\n",
        "  elif train[f].dtype == float:\n",
        "    level = 'interval'\n",
        "  elif train[f].dtype == int:\n",
        "    level = 'ordinal'\n",
        "  \n",
        "  # Initialize keep to True for all variables except for id\n",
        "  keep = True\n",
        "  if f == 'id':\n",
        "    keep = False\n",
        "  \n",
        "  # Defining the data type\n",
        "  dtype = train[f].dtype\n",
        "\n",
        "  # Creating a Dict that contains all the metadata for the variable\n",
        "  f_dict = {\n",
        "      'varname' : f,\n",
        "      'role' : role,\n",
        "      'level' : level,\n",
        "      'keep' : keep,\n",
        "      'dtype' : dtype\n",
        "  }\n",
        "  data.append(f_dict)\n",
        "\n",
        "meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n",
        "meta.set_index('varname', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YJAoE_suiih"
      },
      "source": [
        "meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT73iwcC2XEQ"
      },
      "source": [
        "Example to extract all nminal varialbles that are not dropped"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_OuMnVuifr"
      },
      "source": [
        "meta[(meta.level == 'nominal') & (meta.keep)].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUeu0ywB29bF"
      },
      "source": [
        "Below the number of variables per role and level are displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_hebPzouic0"
      },
      "source": [
        "pd.DataFrame({'count':meta.groupby(['role', 'level'])['role'].size()}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSMGJn_03YN2"
      },
      "source": [
        "# Descriptive statistics\n",
        "\n",
        "We can also apply the describe method on the dataframe. However, it doesn't make much sense to calculate the mean, std, ... on categorical variables and the id variable. We'll explore the categorical variables visually later.\n",
        "\n",
        "Thanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics. To keep things clear, we';; do this per data type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArSww6Qa36UQ"
      },
      "source": [
        "## Interval variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnAZT9afuiaO"
      },
      "source": [
        "v = meta[(meta.level == 'interval') & (meta.keep)].index\n",
        "train[v].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QprY6l4n4-XJ"
      },
      "source": [
        "### reg variables\n",
        "* only ps_reg_03 has missing values\n",
        "* the range (min to max) differs between the variables. We could apply scaling (e.g. StandardScaler), but it depends on the classifier we will want to use.\n",
        "\n",
        "### car variables\n",
        "* ps_car_12 and ps_car_15 have missing values\n",
        "* again, the range differs and we could apply scaling.\n",
        "\n",
        "### calc variables\n",
        "* no missing values\n",
        "* this seems to be some kind of ratio as the maximum is 0.9\n",
        "all three _calc variables have very similar distributions\n",
        "\n",
        "**Overall,** we can see that the range of the interval variables is rather small. Perhaps some transformation (e.g. log) is already applied in order to anonymize the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q0fXtcr5Rs8"
      },
      "source": [
        "## Ordinal variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtdV1p_-uiXY"
      },
      "source": [
        "v = meta[(meta.level == 'ordinal') & (meta.keep)].index\n",
        "train[v].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfJIDggM7Mx2"
      },
      "source": [
        "Only one missing variable: ps_car_11\n",
        "We could apply scaling to deal with the different ranges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nWqfiEm7O6S"
      },
      "source": [
        "## Binary variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWhG6w_-uiUz"
      },
      "source": [
        "v = meta[(meta.level == 'binary') & (meta.keep)].index\n",
        "train[v].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2klmNjsz7ZqO"
      },
      "source": [
        "* A priori in the train data is 3.645%, which is strongly imbalanced.\n",
        "* From the means we can conclude that for most variables the value is zero in most cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENiROnExJWys"
      },
      "source": [
        "# Handling imblaanced classes\n",
        "\n",
        "As we mentioned above the proportion of records with target=1 is far less than target=0. This can lead to a model that has great accuracy but does have any added value in practice. Two possible startegies to deal with tis problem are:\n",
        "* oversampling records with target=1\n",
        "* undersampling records with target=0\n",
        "\n",
        "There are many more starategies of course and MashinLearrningMastery.com gives a nice overview. As we have a rather large training set, we can go for undersampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckc2_ZiJuiSI"
      },
      "source": [
        "desired_apriori = 0.10\n",
        "\n",
        "# Get the indices per target value\n",
        "idx_0 = train[train.target == 0].index\n",
        "idx_1 = train[train.target == 1].index\n",
        "\n",
        "# Get original number of records per target value\n",
        "nb_0 = len(train.loc[idx_0])\n",
        "nb_1 = len(train.loc[idx_1])\n",
        "\n",
        "# Calculate the undersampling rate and resulting number of records with target=0\n",
        "undersampling_rate = ((1-desired_apriori) * nb_1)/(nb_0*desired_apriori)\n",
        "undersampled_nb_0 = int(undersampling_rate * nb_0)\n",
        "print('Rate to undersample records with target=0: {}'.format(undersampling_rate))\n",
        "print('Rate to undersample records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n",
        "\n",
        "# Randomly select records with target=0 to get at the desired a priori\n",
        "undersampled_idx = shuffle(idx_0, random_state=37, n_samples = undersampled_nb_0)\n",
        "\n",
        "# Construct list with remaining indices\n",
        "idx_list = list(undersampled_idx) + list(idx_1)\n",
        "\n",
        "# Return undersample data frame\n",
        "train = train.loc[idx_list].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFOuU0ipMBjn"
      },
      "source": [
        "# Data Quality Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxptNqJIMFcp"
      },
      "source": [
        "## Checking missing values\n",
        "missings are represented as -1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCGIpf2ZuiMw"
      },
      "source": [
        "vars_with_missing = []\n",
        "\n",
        "for f in train.columns:\n",
        "  missings = train[train[f] == -1][f].count()\n",
        "  if missings > 0:\n",
        "    vars_with_missing.append(f)\n",
        "    missings_perc = missings/train.shape[0]\n",
        "\n",
        "    print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
        "\n",
        "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BaJISLCM2VB"
      },
      "source": [
        "* ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.\n",
        "* For the other categorical variables with missing values, we can leave the missing value -1 as such.\n",
        "* ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.\n",
        "* ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.\n",
        "* ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n",
        "* ps_car_14 (continuous) has missing values for 7% of all records. Replace by the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI7AczAuuiKG"
      },
      "source": [
        "# Dropping the variables with too many missing values\n",
        "vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n",
        "train.drop(vars_to_drop, inplace=True, axis=1)\n",
        "meta.loc[(vars_to_drop), 'keep'] = False # Updating the meta\n",
        "\n",
        "# Imputing with the mean or mode\n",
        "mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
        "mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
        "train['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\n",
        "train['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\n",
        "train['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n",
        "train['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qTXwJ13OWp7"
      },
      "source": [
        "## Checking the cardinality of the categorical variables\n",
        "\n",
        "Cardinality refers to the number of different values in a variable. As we will create dummy variables from the categorical variavles later on, we need to check whether there are variables with many distinvt values. We should handle these variables differently as they would result in many dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCT9iCi6uiHe"
      },
      "source": [
        "v = meta[(meta.level == 'nominal') & (meta.keep)].index\n",
        "\n",
        "for f in v:\n",
        "    dist_values = train[f].value_counts().shape[0]\n",
        "    print('Variable {} has {} distinct values'.format(f, dist_values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny1v2graPDFj"
      },
      "source": [
        "Only ps_car_11_cat has many distinct values, although it is still reasonable.\n",
        "\n",
        "EDIT: nickycan made an excellent remark on the fact that my first solution could lead to data leakage. He also pointed me to another kernel made by oliver which deals with that. I therefore replaced this part with the kernel of oliver. All credits go to him. It is so great what you can learn by participating in the Kaggle competitions :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyBScnaZuiFi"
      },
      "source": [
        "# Script by https://www.kaggle.com/ogrellier\n",
        "# Code :  https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
        "\n",
        "def add_noise(series, noise_level):\n",
        "  return series * (1+noise_level * np.random.randn(len(series)))\n",
        "\n",
        "def target_encode(trn_series=None,\n",
        "                  tst_series=None,\n",
        "                  target=None,\n",
        "                  min_samples_leaf=1,\n",
        "                  smoothing=1,\n",
        "                  noise_level=0):\n",
        "  \n",
        "  '''\n",
        "  Smooting is computed like in the following paper by Daniele Micci-Barreca\n",
        "  https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
        "  trn_series : training categorical feature as a pd.Series\n",
        "  tst_series : test categorical feature as a pd.Series\n",
        "  target : target data as a pd.Series\n",
        "  min_samples_leaf (int) : minimum samples to take category average into account\n",
        "  smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
        "  '''\n",
        "  assert len(trn_series) == len(target)\n",
        "  assert trn_series.name == tst_series.name\n",
        "  temp = pd.concat([trn_series, target], axis=1)\n",
        "  # Compute target mean\n",
        "  averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean', 'count'])\n",
        "  # Compute smoothing\n",
        "  smoothing = 1 / (1+np.exp(-(averages['count'] - min_samples_leaf) / smoothing))\n",
        "  # Apply average function to all target data\n",
        "  prior = target.mean()\n",
        "  # The bigger the count the less full_avg is taken into account\n",
        "  averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
        "  averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
        "  # Apply averages to trn and tst series\n",
        "  ft_trn_series = pd.merge(\n",
        "      trn_series.to_frame(trn_series.name),\n",
        "      averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "      on=trn_series.name,\n",
        "      how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "  # pd.merge does not keep the index so restore it\n",
        "  ft_trn_series.index = trn_series.index \n",
        "  ft_tst_series = pd.merge(\n",
        "      tst_series.to_frame(tst_series.name),\n",
        "      averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
        "      on=tst_series.name,\n",
        "      how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
        "  # pd.merge does not keep the index so restore it\n",
        "  ft_tst_series.index = tst_series.index\n",
        "  return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1TV2TcEuiCW"
      },
      "source": [
        "train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n",
        "                             test[\"ps_car_11_cat\"], \n",
        "                             target=train.target, \n",
        "                             min_samples_leaf=100,\n",
        "                             smoothing=10,\n",
        "                             noise_level=0.01)\n",
        "    \n",
        "train['ps_car_11_cat_te'] = train_encoded\n",
        "train.drop('ps_car_11_cat', axis=1, inplace=True)\n",
        "meta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\n",
        "test['ps_car_11_cat_te'] = test_encoded\n",
        "test.drop('ps_car_11_cat', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjgoK1k9Rhwt"
      },
      "source": [
        "# Exploratory Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOhxATnIRl6W"
      },
      "source": [
        "## Categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS6YOS1luh_s"
      },
      "source": [
        "v = meta[(meta.level == 'nominal') & (meta.keep)].index\n",
        "\n",
        "for f in v:\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(figsize=(20,10))\n",
        "    # Calculate the percentage of target=1 per category value\n",
        "    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n",
        "    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n",
        "    # Bar plot\n",
        "    # Order the bars descending on target mean\n",
        "    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n",
        "    plt.ylabel('% target', fontsize=18)\n",
        "    plt.xlabel(f, fontsize=18)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuSSrQBHuh9E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq28FXfLuh6W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3yGtaKuh3-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AEf0ihZuh1T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPX_GLGYuhzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnThhPQfuhxV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6XEEr7DuhuH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXv1u841uhrf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLJb78RWuhpb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3YvkJbauhmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtwPbwzvrnX5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-h5trb0rnVn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1LKRipBrnSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeaQUx4wrnQU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0iporKUrnNq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovDmbHxrnLL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsIYpfe_rnIk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRJTAWI3rnGF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1yenKCHrnDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB4PF3NwrnA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAic5C5yrm-Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWnYVVL9rm73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COsilVsjrm5p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxGwQaQOrm3G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWIoxgU5rm0t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKDoZIhLrmyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-UJj2S4rmv7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}