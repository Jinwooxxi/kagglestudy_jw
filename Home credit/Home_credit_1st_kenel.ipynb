{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Home credit_1st_kenel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRimEsleZQOWxVgvdwoynT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinwooxxi/kagglestudy_jw/blob/main/Home%20credit/Home_credit_1st_kenel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFG_aakfi7T5"
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2zwkd-ajK3u"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c home-credit-default-risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzW4cJh0jzI7"
      },
      "source": [
        "# numpy and pandas for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "# sklearn preprocessing for dealing with categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# File system manangement\n",
        "import os\n",
        "\n",
        "# Suppress warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# matplotlib and seaborn for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtxUDCYSjPP3"
      },
      "source": [
        "# List files available\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGQKiX0ElljI"
      },
      "source": [
        "!unzip application_train.csv.zip\n",
        "!unzip application_test.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvJlZBVIj1FO"
      },
      "source": [
        "# Training data\n",
        "app_train = pd.read_csv('application_train.csv')\n",
        "print('Training data shape: ', app_train.shape)\n",
        "app_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAqz82mXlTdB"
      },
      "source": [
        "# Testing data features\n",
        "app_test = pd.read_csv('application_test.csv')\n",
        "print('Testing data shape: ', app_test.shape)\n",
        "app_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpEw3wZXqt3M"
      },
      "source": [
        "# EDA\n",
        "Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dKVouRZq0sL"
      },
      "source": [
        "## Examine the Distribution of the Target Column\n",
        "The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noQT3Vfnq8Fq"
      },
      "source": [
        "print(app_train['TARGET'].value_counts())\n",
        "app_train['TARGET'].astype(int).plot.hist();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1utLxs3ArMSr"
      },
      "source": [
        "From this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dft3SzB_rQGO"
      },
      "source": [
        "## Examine Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDBtjJZZrP1j"
      },
      "source": [
        "# Function to calculate missing values by column# Funct\n",
        "def missing_values_table(df):\n",
        "  # Total missing values\n",
        "  mis_val = df.isna().sum()\n",
        "\n",
        "  # Percentage of missing values\n",
        "  mis_val_percent = 100 * df.isna().sum() / len(df)\n",
        "\n",
        "  # Make a table with the results\n",
        "  mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "  # Rename the columns\n",
        "  mis_val_table_ren_columns = mis_val_table.rename(columns={0:'Missing Values', 1:'% of Total Values'})\n",
        "\n",
        "  # Sort the table by percentage of missing descending\n",
        "  mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] !=0].sort_values('% of Total Values', ascending=False).round(1)\n",
        "\n",
        "  # Print some summary information\n",
        "  print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values.\")\n",
        "        \n",
        "  # Return the dataframe with missing information\n",
        "  return mis_val_table_ren_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFMxiXzmq8DU"
      },
      "source": [
        "missing_values = missing_values_table(app_train)\n",
        "missing_values.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIkfzKeMs6P5"
      },
      "source": [
        "When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcF7iAvJs7Fj"
      },
      "source": [
        "## Column Types\n",
        "Let's look at the number of columns of each data type. `int64` and `float64` are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DhvtVLDq8BD"
      },
      "source": [
        "# Number of each type of column\n",
        "app_train.dtypes.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEXG-8Ijq7-q"
      },
      "source": [
        "# Number of unique classes in each object column\n",
        "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq6juP7RtMsa"
      },
      "source": [
        "## Encoding Categorical Variables\n",
        "Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n",
        "\n",
        "* Label encoding: assign each unique category in a categorical variable with an integer. No new columns are created. An example is shown below\n",
        "* One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n",
        "\n",
        "The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\n",
        "\n",
        "There is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. Here is a good Stack Overflow discussion. I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).\n",
        "\n",
        "In this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2GsU4FwtcTs"
      },
      "source": [
        "### Label Encoding and One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT5Pq0mcq78S"
      },
      "source": [
        "# Create a label encoder object\n",
        "le = LabelEncoder()\n",
        "le_count = 0\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in app_train:\n",
        "  if app_train[col].dtype == 'object':\n",
        "    # If 2 or fewer unique categories\n",
        "    if len(list(app_train[col].unique())) <=2:\n",
        "      # Train on the training data\n",
        "      le.fit(app_train[col])\n",
        "      # Transform both training and testing data\n",
        "      app_train[col] = le.transform(app_train[col])\n",
        "      app_test[col] = le.transform(app_test[col])\n",
        "\n",
        "      # Keep track of how many columns were label encoded\n",
        "      le_count +=1\n",
        "\n",
        "print('%d columns were label encoded.' % le_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isbUYNlOq75r"
      },
      "source": [
        "# one-hot encoding of categorical variables\n",
        "app_train = pd.get_dummies(app_train)\n",
        "app_test = pd.get_dummies(app_test)\n",
        "\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWpIAKWsueyA"
      },
      "source": [
        "### Aligning Training and Testing Data\n",
        "There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the `align`, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSZ8_73Dq73W"
      },
      "source": [
        "train_labels = app_train['TARGET']\n",
        "\n",
        "# Align the training and testing data, keep only columns present in both dataframes\n",
        "app_train, app_test = app_train.align(app_test, join='inner', axis = 1)\n",
        "\n",
        "# Add the target back in\n",
        "app_train['TARGET'] = train_labels\n",
        "\n",
        "print('Training Features shape: ', app_train.shape)\n",
        "print('Testing Features shape: ', app_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2nIirEGq70_"
      },
      "source": [
        "app_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zZIwIBZxhum"
      },
      "source": [
        "## Back to EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASRiFkWxoxW"
      },
      "source": [
        "### Anomalies\n",
        "One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkXTYZmFxn_Y"
      },
      "source": [
        "(app_train['DAYS_BIRTH'] / -365).describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNciFZOzq7yo"
      },
      "source": [
        "app_train['DAYS_EMPLOYED'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuEC_3Tlx7zu"
      },
      "source": [
        "That doesn't look right! The maximum value (besides being positive) is about 1000 years!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKn0vYvKq7wB"
      },
      "source": [
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
        "plt.xlabel('Days Employment');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltAoZsz4q7ot"
      },
      "source": [
        "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
        "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
        "print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n",
        "print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n",
        "print('There are %d anomalous days of employment' % len(anom))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_0tQC42yOj9"
      },
      "source": [
        "Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\n",
        "\n",
        "Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQYnmaO3mIqF"
      },
      "source": [
        "# Create an anomalous flag column\n",
        "app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n",
        "\n",
        "# Replace the anomalous values with nan\n",
        "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
        "plt.xlabel('Days Employment');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8945k2nyVD2"
      },
      "source": [
        "The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers.\n",
        "\n",
        "As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymCkH85AyREw"
      },
      "source": [
        "app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n",
        "app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n",
        "\n",
        "print('There are %d anomalies in the test data out of %d entries.' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDocCtIyyfYu"
      },
      "source": [
        "### Corrlations\n",
        "\n",
        "\n",
        "Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n",
        "\n",
        "The correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n",
        "\n",
        "* .00-.19 “very weak”\n",
        "* .20-.39 “weak”\n",
        "* .40-.59 “moderate”\n",
        "* .60-.79 “strong”\n",
        "* .80-1.0 “very strong”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZwgb0iyXLr"
      },
      "source": [
        "# Find correlations with the target and sort\n",
        "correlations = app_train.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display correlations\n",
        "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
        "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lrPYGb3zTIF"
      },
      "source": [
        "Let's take a look at some of more significant correlations: the `DAYS_BIRTH` is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, `DAYS_BIRTH` is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svoAJS50zXpL"
      },
      "source": [
        "### Effect of Age on Repayment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miPjAWtzynt3"
      },
      "source": [
        "# Find the correlation of the positive days since birth and target\n",
        "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
        "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdcdXVEXzgEf"
      },
      "source": [
        "As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.\n",
        "\n",
        "Let's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWcwPKeyzc3l"
      },
      "source": [
        "# Set the style of plots\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Plot the distribution of ages in years\n",
        "plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 50)\n",
        "plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NngS3oDTWXAP"
      },
      "source": [
        "By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_kZ0Fc9ziBr"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# KDE plot of loans that were repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target==0')\n",
        "\n",
        "# KDE plot of loans that were not repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target==1')\n",
        "\n",
        "# Labeling of plot\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Ages');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au6ck3IRXNs_"
      },
      "source": [
        "The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\n",
        "\n",
        "To make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvG3w3yW4-n"
      },
      "source": [
        "# Age information into a separate dataframe\n",
        "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
        "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
        "\n",
        "# Bin the age data\n",
        "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
        "age_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Riu_Fl9XPeq"
      },
      "source": [
        "# Group by the bin and calculate averages\n",
        "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
        "age_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4HW7l3XRh9"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "\n",
        "# Graph the age bins and the average of the target as a bar plot\n",
        "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
        "\n",
        "# Plot labeling\n",
        "plt.xticks(rotation = 75)\n",
        "plt.xlabel('Age Group (years)')\n",
        "plt.ylabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Age Group');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31jXySSrXeqX"
      },
      "source": [
        "There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n",
        "\n",
        "This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsvh7jyCVq9D"
      },
      "source": [
        "Exterior Sources\n",
        "The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`. According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.\n",
        "\n",
        "Let's take a look at these variables.\n",
        "\n",
        "First, we can show the correlations of the `EXT_SOURCE` features with the target and with each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6oa9qyJXXsa"
      },
      "source": [
        "# Extract the EXT_SOURCE variables and show corrlations\n",
        "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "ext_data_corrs = ext_data.corr()\n",
        "ext_data_corrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If9ZARDuV8mO"
      },
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "\n",
        "# Heatmap of correlations\n",
        "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
        "plt.title('Correlation Heatmap');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAP4dLJ1WDID"
      },
      "source": [
        "plt.figure(figsize = (10, 12))\n",
        "\n",
        "# iterate through the sources\n",
        "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
        "    \n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(3, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
        "    \n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % source)\n",
        "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
        "    \n",
        "plt.tight_layout(h_pad = 2.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2IIga8XZux"
      },
      "source": [
        "## Pair plot\n",
        "\n",
        "As a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n",
        "\n",
        "If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYb8fHiwW8yT"
      },
      "source": [
        "# Copy the data for plotting\n",
        "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
        "\n",
        "# Add in the age of the client in years\n",
        "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
        "\n",
        "# Drop na values and limit to first 100000 rows\n",
        "plot_data = plot_data.dropna().loc[:100000, :]\n",
        "\n",
        "# Function to calculate correlation coefficient between two columns\n",
        "def corr_func(x, y, **kwargs):\n",
        "    r = np.corrcoef(x, y)[0][1]\n",
        "    ax = plt.gca()\n",
        "    ax.annotate(\"r = {:.2f}\".format(r),\n",
        "                xy=(.2, .8), xycoords=ax.transAxes,\n",
        "                size = 20)\n",
        "\n",
        "# Create the pairgrid object\n",
        "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False, hue = 'TARGET', \n",
        "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
        "\n",
        "# Upper is a scatter plot\n",
        "grid.map_upper(plt.scatter, alpha = 0.2)\n",
        "\n",
        "# Diagonal is a histogram\n",
        "grid.map_diag(sns.kdeplot)\n",
        "\n",
        "# Bottom is density plot\n",
        "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
        "\n",
        "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4kf8mRXC74i"
      },
      "source": [
        "In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2CTHq1NDNvR"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "We will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods:\n",
        "\n",
        "* Polynomial features\n",
        "* Domain knowledge features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3LeiVAWHhhg"
      },
      "source": [
        "## Polynomal Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exWA7YlYXqQC"
      },
      "source": [
        "# Make a new DataFrame for polynomial feature\n",
        "poly_features = app_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH', 'TARGET']]\n",
        "poly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\n",
        "\n",
        "# imputer for handling missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features['TARGET']\n",
        "\n",
        "poly_features = poly_features.drop(columns = ['TARGET'])\n",
        "\n",
        "# Need to impute missing values\n",
        "poly_features = imputer.fit_transform(poly_features)\n",
        "poly_features_test = imputer.transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "                                  \n",
        "# Create the polynomial object with specified degree\n",
        "poly_transformer = PolynomialFeatures(degree = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9nYEFWlExKA"
      },
      "source": [
        "# Train the polynomial features\n",
        "poly_transformer.fit(poly_features)\n",
        "\n",
        "# Transform the features\n",
        "poly_features = poly_transformer.transform(poly_features)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)\n",
        "print('Polynomial Features shape: ', poly_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KLqo-5vFLHI"
      },
      "source": [
        "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS67Jnf4FO7J"
      },
      "source": [
        "# Create a DataFrame of the feature\n",
        "poly_features = pd.DataFrame(poly_features, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1',\n",
        "                                                                                        'EXT_SOURCE_2',\n",
        "                                                                                        'EXT_SOURCE_3',\n",
        "                                                                                        'DAYS_BIRTH']))\n",
        "\n",
        "# add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "# Find the correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most negative and most positive\n",
        "print(poly_corrs.head(10))\n",
        "print()\n",
        "print(poly_corrs.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VD-ogXFF5ek"
      },
      "source": [
        "# Put test featrues into DataFrame\n",
        "poly_features_test = pd.DataFrame(poly_features_test,\n",
        "                                  columns=poly_transformer.get_feature_names(['EXT_SOURCE_1',\n",
        "                                                                              'EXT_SOURCE_2',\n",
        "                                                                              'EXT_SOURCE_3',\n",
        "                                                                              'DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features into training dataframe\n",
        "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
        "app_train_poly = app_train.merge(poly_features, on='SK_ID_CURR', how='left')\n",
        "\n",
        "# merge polynimal features into testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
        "app_test_poly = app_test.merge(poly_features, on='SK_ID_CURR', how='left')\n",
        "\n",
        "# Align the dataframes\n",
        "app_train_poly, app_test_poly = app_train.align(app_test_poly, join='inner', axis=1)\n",
        "\n",
        "# Print out the new shapes\n",
        "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
        "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I87CduMNHkv_"
      },
      "source": [
        "## Domain Knowledge Features\n",
        "\n",
        "* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n",
        "* ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n",
        "* CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n",
        "* DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6TTlGQVHbug"
      },
      "source": [
        "app_train_domain = app_train.copy()\n",
        "app_test_domain = app_test.copy()\n",
        "\n",
        "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
        "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
        "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n",
        "\n",
        "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
        "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
        "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyXT6ZSOIyoi"
      },
      "source": [
        "plt.figure(figsize = (12, 20))\n",
        "# iterate through the new features\n",
        "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
        "    \n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
        "    \n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % feature)\n",
        "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
        "    \n",
        "plt.tight_layout(h_pad = 2.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9qNW4LI82a"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5).\n",
        "\n",
        "Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1nBNc5IJF9p"
      },
      "source": [
        "## Logistic Regression Implementataion\n",
        "To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSV1E3P8I1Of"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Drop the target from the training data\n",
        "if 'TARGET' in app_train:\n",
        "    train = app_train.drop(columns = ['TARGET'])\n",
        "else:\n",
        "    train = app_train.copy()\n",
        "    \n",
        "# Feature names\n",
        "features = list(train.columns)\n",
        "\n",
        "# Copy of the testing data\n",
        "test = app_test.copy()\n",
        "\n",
        "# Median imputation of missing values\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "# Scale each feature to 0-1\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "# Fit on the training data\n",
        "imputer.fit(train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "train = imputer.transform(train)\n",
        "test = imputer.transform(app_test)\n",
        "\n",
        "# Repeat with the scaler\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "test = scaler.transform(test)\n",
        "\n",
        "print('Training data shape: ', train.shape)\n",
        "print('Testing data shape: ', test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdBRkli6J7s4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Make the model with the specified regularization parameter\n",
        "log_reg = LogisticRegression(C = 0.0001)\n",
        "\n",
        "# Train on the training data\n",
        "log_reg.fit(train, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3RewGiPKGDo"
      },
      "source": [
        "# Make predictions\n",
        "# Make sure to select the second column only\n",
        "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knfzj1NWKItS"
      },
      "source": [
        "# Submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = log_reg_pred\n",
        "\n",
        "submit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaVVfqvAl682"
      },
      "source": [
        "The logistic regression baseline should score around 0.671 when submitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGz3NswEKMYD"
      },
      "source": [
        "## Improved Medel : Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXXmuHrFKKml"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Make the random forest classifier\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n",
        "\n",
        "# Train on the training data\n",
        "random_forest.fit(train, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values = random_forest.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest.predict_proba(test)[:, 1]\n",
        "\n",
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee612400l9_Y"
      },
      "source": [
        "This model should score around 0.678 when submitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5U1R-Pdl-9V"
      },
      "source": [
        "### Make Predictions using Engineered Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D7K2zWAKWWu"
      },
      "source": [
        "poly_features_names = list(app_train_poly.columns)\n",
        "\n",
        "# Impute the polynomial features\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "poly_features = imputer.fit_transform(app_train_poly)\n",
        "poly_features_test = imputer.transform(app_test_poly)\n",
        "\n",
        "# Scale the polynomial features\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "poly_features = scaler.fit_transform(poly_features)\n",
        "poly_features_test = scaler.transform(poly_features_test)\n",
        "\n",
        "random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8T0uTyamGmt"
      },
      "source": [
        "# Train on the training data\n",
        "random_forest_poly.fit(poly_features, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxNTDgjemGdC"
      },
      "source": [
        "# Make a submission dataframe\n",
        "submit = app_test[['SK_ID_CURR']]\n",
        "submit['TARGET'] = predictions\n",
        "\n",
        "# Save the submission dataframe\n",
        "submit.to_csv('random_forest_baseline_engineered.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJcbjuC-ntqS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}